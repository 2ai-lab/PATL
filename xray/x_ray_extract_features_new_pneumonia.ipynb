{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Tenforflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "from tensorflow.keras.applications.densenet import DenseNet169\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read image file paths and store to a list both for Pneumonia and Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Files Count\n",
      "Normal: 2400\n",
      "pneumonia: 695\n",
      "Total: 3095\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../../dataset/xray_new/covid_19_Radiography_Dataset_Refined/\"\n",
    "\n",
    "all_files = []\n",
    "\n",
    "pneumonia_files = glob.glob(os.path.join(dataset_path, \"train\", \"Viral Pneumonia\", \"images\", \"*.png\"))\n",
    "pneumonia_files.extend(glob.glob(os.path.join(dataset_path, \"test\", \"Viral Pneumonia\", \"images\", \"*.png\")))\n",
    "\n",
    "normal_files = glob.glob(os.path.join(dataset_path, \"train\", \"Normal\", \"images\",\"*.png\"))\n",
    "normal_files.extend(glob.glob(os.path.join(dataset_path, \"test\", \"Normal\", \"images\", \"*.png\")))\n",
    "\n",
    "# Get all the files from the directory in a two element list.\n",
    "# First element is list of file location to pneumonia images and second element is list of file location to Normal images.\n",
    "all_files = [normal_files, pneumonia_files]\n",
    "print(\"Image Files Count\\nNormal: {}\\npneumonia: {}\\nTotal: {}\".format(len(normal_files), len(pneumonia_files), len(all_files[0] + all_files[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = 800\n",
    "pneumonia_count = 695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read image from file list and store the corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(files, label, data_count, img_size, image_id_count):\n",
    "  dataset = []  # List to hold all the dataset. Each element is a dictionary\n",
    "\n",
    "  count = 1\n",
    "  for j in tqdm(files):  # Loop over each file location\n",
    "    data_dict = {}\n",
    "    data_dict[\"id\"] = image_id_count\n",
    "    data_dict[\"filepath\"] = j\n",
    "    try:\n",
    "      img = cv2.imread(j)\n",
    "      img = cv2.resize(img, (img_size, img_size))\n",
    "      data_dict[\"image\"] = img\n",
    "      data_dict[\"label\"] = label\n",
    "      dataset.append(data_dict)\n",
    "      if count == data_count:\n",
    "        break\n",
    "      count += 1\n",
    "      image_id_count += 1\n",
    "    except Exception as e:\n",
    "      print(\"faulty image: {} {}\".format(j, e))\n",
    "  return dataset, image_id_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read image from the file and store the corresponding label in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▎                                                    | 799/2400 [00:14<00:29, 53.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▉| 694/695 [00:09<00:00, 72.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Count\n",
      "Normal: 800\n",
      "pneumonia: 695\n",
      "Total: 1495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c_dataset, nc_dataset, t_dataset = [], [], []\n",
    "image_id_count = 1       # counter to record the ids of files. Each file has a unique ID.\n",
    "img_size = 224\n",
    "# all_files => [NC, C]\n",
    "for i, data in enumerate(all_files): # only two loops for pneumonia and Normal\n",
    "  if i == 0:\n",
    "    nc_dataset, image_id_count = get_dataset(data, i, normal_count, img_size, image_id_count)\n",
    "  else:\n",
    "    c_dataset, image_id_count = get_dataset(data, i, pneumonia_count, img_size, image_id_count)\n",
    "tot_dataset = nc_dataset + c_dataset\n",
    "print(\"Dataset Count\\nNormal: {}\\npneumonia: {}\\nTotal: {}\".format(len(nc_dataset), len(c_dataset), len(tot_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image only from the dataset to send to DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only = []\n",
    "for data in tot_dataset:\n",
    "  image_only.append(data[\"image\"])\n",
    "image_only = np.array(image_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495\n"
     ]
    }
   ],
   "source": [
    "batch_size = image_only.shape[0]\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches of images to feed into DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_img = img_datagen.flow(image_only, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to select a model from three (VGG16, ResNet101 and DenseNet169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models(img_size, model_sel):\n",
    "  if model_sel == 1:\n",
    "    vgg_pre_t = VGG16(input_shape=(img_size, img_size, 3), include_top=False, weights ='imagenet')\n",
    "    return vgg_pre_t, 25088\n",
    "\n",
    "  elif model_sel == 2:\n",
    "    resnet_pre_t= ResNet101(input_shape=(img_size, img_size, 3), include_top=False, weights='imagenet')\n",
    "    return resnet_pre_t, 100352\n",
    "\n",
    "  elif model_sel == 3:\n",
    "    densenet169_pre_t = DenseNet169(input_shape=(img_size, img_size, 3), include_top=False, weights ='imagenet' )\n",
    "    return densenet169_pre_t, 81536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model among 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model = 1 # int(input(\"Enter the number for: \\n 1) VGG16 \\n 2) Resnet101  \\n 3) Densenet169 \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image feature from the selected DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 24s 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:40<00:00, 40.79s/it]\n"
     ]
    }
   ],
   "source": [
    "all_features, reduced_features = [], []\n",
    "pca = PCA(n_components=batch_size)\n",
    "\n",
    "all_feat = []\n",
    "model, feature_size = all_models(img_size, select_model)\n",
    "for data in tqdm(range(len(batch_img))):\n",
    "  try:\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(batch_size, feature_size)\n",
    "    feature_matrix = features.reshape(features.shape[0], -1)\n",
    "    reduced_features = pca.fit_transform(feature_matrix)\n",
    "  except:\n",
    "    img_len = len(batch_img[data])\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(img_len, feature_size)\n",
    "    feature_matrix = features.reshape(features.shape[0], -1)\n",
    "    reduced_features = pca.fit_transform(feature_matrix)\n",
    "  all_feat.extend(reduced_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace image value by image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1495\n",
      "{'id': 1, 'filepath': '../../dataset/xray_new/covid_19_Radiography_Dataset_Refined/train\\\\Normal\\\\images\\\\Normal-1000.png', 'image': array([-1.0655748e+01,  2.6115465e+00, -5.2301559e+00, ...,\n",
      "        6.1773482e-11,  6.5538365e-12,  2.4485494e-12], dtype=float32), 'label': 0}\n",
      "{'id': 900, 'filepath': '../../dataset/xray_new/covid_19_Radiography_Dataset_Refined/train\\\\Viral Pneumonia\\\\images\\\\Viral Pneumonia-1100.png', 'image': array([3.3517089e+00, 5.7182102e+00, 2.8066650e-01, ..., 6.2010910e-11,\n",
      "       6.7252532e-12, 2.3683824e-12], dtype=float32), 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tot_dataset)):\n",
    "  tot_dataset[i]['image'] = all_feat[i]\n",
    "\n",
    "# print sample dataset pneumonia and Normal\n",
    "print(len(tot_dataset))\n",
    "print(tot_dataset[0])\n",
    "print(tot_dataset[900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save extracted feature in pickle file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/x_ray/pneumonia\"\n",
    "if select_model == 1:\n",
    "  filename = f\"x_ray_pca_{len(tot_dataset)}_vgg16.pickle\"\n",
    "elif select_model == 2:\n",
    "  filename = f\"x_ray_pca_{len(tot_dataset)}_resnet101.pickle\"\n",
    "elif select_model == 3:\n",
    "  filename = f\"x_ray_pca_{len(tot_dataset)}_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'wb') as handle:\n",
    "  pickle.dump(tot_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
